{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23efa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2176, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5680, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 100) vs (None, 10, 10, 1)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17884\\2848302084.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17884\\2848302084.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# Step 4: Train the neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;31m# Step 5: Validation (evaluate the model on the validation set if needed)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17884\\2848302084.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0my_train_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Scale the target values between 0 and 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_val_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2176, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"C:\\Users\\nagan\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5680, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 100) vs (None, 10, 10, 1)).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt  # Add this line at the beginning of your script\n",
    "\n",
    "\n",
    "#file_path = '/home/ngandur/Forensic/US_Accidents_March23.csv'\n",
    "file_path = '/home/ngandur/Forensic/US_Accidents_March23.csv'\n",
    "grid_size = 10\n",
    "print (1)\n",
    "\n",
    "def preprocess_data_for_year(file_path, year):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Start_Time'] = pd.to_datetime(data['Start_Time'])\n",
    "    data_year = data[data['Start_Time'].dt.year == year]\n",
    "    \n",
    "    sample_size = len(data_year) - 1\n",
    "    indices = np.random.choice(len(data_year), sample_size, replace=False)\n",
    "    lat = data_year['Start_Lat'].iloc[indices]\n",
    "    lon = data_year['Start_Lng'].iloc[indices]\n",
    "    data_sampled = np.column_stack((lat, lon))\n",
    "    kde = gaussian_kde(data_sampled.T)\n",
    "    \n",
    "    lat_min, lat_max = lat.min(), lat.max()\n",
    "    lon_min, lon_max = lon.min(), lon.max()\n",
    "    lat_grid = np.linspace(lat_min, lat_max, grid_size)\n",
    "    lon_grid = np.linspace(lon_min, lon_max, grid_size)\n",
    "    lat_mesh, lon_mesh = np.meshgrid(lat_grid, lon_grid)\n",
    "    positions = np.vstack([lat_mesh.ravel(), lon_mesh.ravel()])\n",
    "    pdf_values = kde(positions)\n",
    "    pdf_values = pdf_values.reshape(lat_mesh.shape)\n",
    "    \n",
    "    return lat_mesh, lon_mesh, pdf_values\n",
    "\n",
    "print (2)\n",
    "\n",
    "def create_dataset(file_path, years):\n",
    "    X = []\n",
    "    y = []\n",
    "    for year in years:\n",
    "        lat_mesh, lon_mesh, pdf_values = preprocess_data_for_year(file_path, year)\n",
    "        X_year = np.stack((lat_mesh, lon_mesh), axis=-1)  # Combine latitude and longitude\n",
    "        X.append(X_year)\n",
    "        y.append(pdf_values)\n",
    "    return np.array(X), np.array(y)  # Convert y to a numpy array\n",
    "\n",
    "print (3)\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    input_shape = (grid_size, grid_size, 2, 1)  # Update input shape\n",
    "    model.add(Conv3D(16, kernel_size=(3, 3, 1), activation='relu', input_shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(grid_size * grid_size, activation='relu'))\n",
    "    model.add(Dense(grid_size * grid_size, activation='sigmoid'))  # You can keep sigmoid activation here\n",
    "    return model\n",
    "\n",
    "\n",
    "print (4)\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    model = build_model()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')  # Use mean squared error loss for regression\n",
    "    y_train_scaled = y_train / y_train.max()  # Scale the target values between 0 and 1\n",
    "    y_val_scaled = y_val / y_val.max()\n",
    "    model.fit(X_train, y_train_scaled, epochs=20, batch_size=32, validation_data=(X_val, y_val_scaled))\n",
    "    \n",
    "    \n",
    "    # Predict using the model\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reshape predictions to match the target shape\n",
    "    y_pred_train = y_pred_train.reshape(-1, grid_size * grid_size)  # Flatten to (None, 100)\n",
    "    y_pred_val = y_pred_val.reshape(-1, grid_size * grid_size)      # Flatten to (None, 100)\n",
    "    \n",
    "    # Calculate the mean squared error\n",
    "    train_loss = np.mean(np.square(y_train - y_pred_train))\n",
    "    val_loss = np.mean(np.square(y_val - y_pred_val))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "print (5)\n",
    "\n",
    "def plot_heatmap_year(file_path, year, model):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert 'Start_Time' column to datetime object\n",
    "    data['Start_Time'] = pd.to_datetime(data['Start_Time'])\n",
    "\n",
    "    # Filter the data for the given year\n",
    "    data_year = data[data['Start_Time'].dt.year == year]\n",
    "\n",
    "    # Downsample the data to reduce the number of points\n",
    "    sample_size = len(data_year) - 1\n",
    "    indices = np.random.choice(len(data_year), sample_size, replace=False)\n",
    "    lat1 = data_year['Start_Lat'].iloc[indices]\n",
    "    lon1 = data_year['Start_Lng'].iloc[indices]\n",
    "\n",
    "    # Create a kernel density estimate for latitude and longitude\n",
    "    data_sampled = np.column_stack((lat1, lon1))\n",
    "    kde = gaussian_kde(data_sampled.T)\n",
    "\n",
    "    # Define the grid for the 3D plot\n",
    "    lat_min, lat_max = lat1.min(), lat1.max()\n",
    "    lon_min, lon_max = lon1.min(), lon1.max()\n",
    "    lat_grid = np.linspace(lat_min, lat_max, grid_size)\n",
    "    lon_grid = np.linspace(lon_min, lon_max, grid_size)\n",
    "    lat_mesh, lon_mesh = np.meshgrid(lat_grid, lon_grid)\n",
    "\n",
    "    # Calculate the PDF values using the pre-trained model\n",
    "    positions = np.stack((lat_mesh, lon_mesh), axis=-1)  # Combine latitude and longitude\n",
    "    positions = positions.reshape(1, grid_size, grid_size, 2, 1)  # Update input shape\n",
    "    predicted_pdf = model.predict(positions)\n",
    "    \n",
    "    # Reshape predicted_pdf to match the shape of the target values\n",
    "    predicted_pdf = predicted_pdf.reshape(-1, grid_size, grid_size, 1)\n",
    "    \n",
    "    # Calculate the PDF values using the pre-trained model\n",
    "    pdf_values = predicted_pdf.reshape(lat_mesh.shape)\n",
    "\n",
    "    # Find the indices of the maximum PDF value in the grid\n",
    "    max_index = np.argmax(pdf_values)\n",
    "    lat_index, lon_index = np.unravel_index(max_index, lat_mesh.shape)\n",
    "    lat_max_value = lat_mesh[lat_index, lon_index]\n",
    "    lon_max_value = lon_mesh[lat_index, lon_index]\n",
    "\n",
    "    # Create a 3D plot\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot the PDF surface\n",
    "    ax.plot_surface(lat_mesh, lon_mesh, pdf_values, cmap='coolwarm', alpha=0.7)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Latitude')\n",
    "    ax.set_ylabel('Longitude')\n",
    "    ax.set_zlabel('Probability Density')\n",
    "    ax.set_title(f'{year} Probability Density Function (PDF) of Latitude and Longitude')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "print (6)    \n",
    "\n",
    "def main():\n",
    "    # Step 1: Create datasets for each year\n",
    "    #file_path = '/home/ngandur/Forensic/US_Accidents_March23.csv'   \n",
    "    file_path = r'C:\\Users\\nagan\\OneDrive\\√Årea de Trabalho\\PhD\\Classes\\Fall 2023\\Capstone\\US_Accidents_March23.csv'\n",
    "    #years = [2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
    "    years = [2016, 2017]\n",
    "    X, y = create_dataset(file_path, years)\n",
    "\n",
    "    # Step 2: Define grid size Already defined\n",
    "\n",
    "\n",
    "    # Step 3: Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Reshape y_train and y_val to match the shape of the predictions\n",
    "    #y_train = y_train.reshape(-1, grid_size, grid_size, 1, 1, 1, 1, 1, 1)  # Add an extra dimension for the last axis\n",
    "    #y_val = y_val.reshape(-1, grid_size, grid_size, 1, 1, 1, 1, 1, 1)  # Add an extra dimension for the last axis\n",
    "    y_train = y_train.reshape(-1, grid_size, grid_size, 1)  # Add an extra dimension for the last axis\n",
    "    y_val = y_val.reshape(-1, grid_size, grid_size, 1)  # Add an extra dimension for the last axis\n",
    "    \n",
    "    # Step 4: Train the neural network\n",
    "    model = train_model(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # Step 5: Validation (evaluate the model on the validation set if needed)\n",
    "\n",
    "    # Step 6: Extrapolation to future years (2023, 2024, 2025)\n",
    "    future_years = [2023]\n",
    "    for future_year in future_years:\n",
    "        X_future, _ = create_dataset(file_path, [future_year])\n",
    "        predicted_pdf = model.predict(X_future)\n",
    "        # Process the predicted PDF as needed for visualization or further analysis\n",
    "\n",
    "    \n",
    "        # Create a 3D plot for the extrapolation year\n",
    "        plot_heatmap_year(file_path, future_year, model)\n",
    "    \n",
    "print (7)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
